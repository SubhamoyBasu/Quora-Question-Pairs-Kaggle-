{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quora Question Pairs (Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_NB_WORDS = 200000\n",
    "MAX_SEQUENCE_LENGTH = 25\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean text\n",
    "train_data['q1_processed'] = train_data['question1'].apply(lambda x: re.sub(r\"what's\", \"what is \",str(x)))\n",
    "train_data['q1_processed'] = train_data['question1'].apply(lambda x: re.sub(r\"\\'s\", \" \",str(x)))\n",
    "train_data['q1_processed'] = train_data['question1'].apply(lambda x: re.sub(r\"\\'ve\", \" have \",str(x)))\n",
    "train_data['q1_processed'] = train_data['question1'].apply(lambda x: re.sub(r\"can't\", \"cannot \",str(x)))\n",
    "train_data['q1_processed'] = train_data['question1'].apply(lambda x: re.sub(r\"n't\", \" not \",str(x)))\n",
    "train_data['q1_processed'] = train_data['question1'].apply(lambda x: re.sub(r\"i'm\", \"i am \",str(x)))\n",
    "train_data['q1_processed'] = train_data['question1'].apply(lambda x: re.sub(r\"\\'re\", \" are \",str(x)))\n",
    "train_data['q1_processed'] = train_data['question1'].apply(lambda x: re.sub(r\"\\'d\", \" would \",str(x)))\n",
    "train_data['q1_processed'] = train_data['question1'].apply(lambda x: re.sub(r\"\\'ll\", \" will \",str(x)))\n",
    "\n",
    "train_data['q2_processed'] = train_data['question2'].apply(lambda x: re.sub(r\"what's\", \"what is \",str(x)))\n",
    "train_data['q2_processed'] = train_data['question2'].apply(lambda x: re.sub(r\"\\'s\", \" \",str(x)))\n",
    "train_data['q2_processed'] = train_data['question2'].apply(lambda x: re.sub(r\"\\'ve\", \" have \",str(x)))\n",
    "train_data['q2_processed'] = train_data['question2'].apply(lambda x: re.sub(r\"can't\", \"cannot \",str(x)))\n",
    "train_data['q2_processed'] = train_data['question2'].apply(lambda x: re.sub(r\"n't\", \" not \",str(x)))\n",
    "train_data['q2_processed'] = train_data['question2'].apply(lambda x: re.sub(r\"i'm\", \"i am \",str(x)))\n",
    "train_data['q2_processed'] = train_data['question2'].apply(lambda x: re.sub(r\"\\'re\", \" are \",str(x)))\n",
    "train_data['q2_processed'] = train_data['question2'].apply(lambda x: re.sub(r\"\\'d\", \" would \",str(x)))\n",
    "train_data['q2_processed'] = train_data['question2'].apply(lambda x: re.sub(r\"\\'ll\", \" will \",str(x)))\n",
    "\n",
    "test_data['q1_processed'] = test_data['question1'].apply(lambda x: re.sub(r\"what's\", \"what is \",str(x)))\n",
    "test_data['q1_processed'] = test_data['question1'].apply(lambda x: re.sub(r\"\\'s\", \" \",str(x)))\n",
    "test_data['q1_processed'] = test_data['question1'].apply(lambda x: re.sub(r\"\\'ve\", \" have \",str(x)))\n",
    "test_data['q1_processed'] = test_data['question1'].apply(lambda x: re.sub(r\"can't\", \"cannot \",str(x)))\n",
    "test_data['q1_processed'] = test_data['question1'].apply(lambda x: re.sub(r\"n't\", \" not \",str(x)))\n",
    "test_data['q1_processed'] = test_data['question1'].apply(lambda x: re.sub(r\"i'm\", \"i am \",str(x)))\n",
    "test_data['q1_processed'] = test_data['question1'].apply(lambda x: re.sub(r\"\\'re\", \" are \",str(x)))\n",
    "test_data['q1_processed'] = test_data['question1'].apply(lambda x: re.sub(r\"\\'d\", \" would \",str(x)))\n",
    "test_data['q1_processed'] = test_data['question1'].apply(lambda x: re.sub(r\"\\'ll\", \" will \",str(x)))\n",
    "\n",
    "test_data['q2_processed'] = test_data['question2'].apply(lambda x: re.sub(r\"what's\", \"what is \",str(x)))\n",
    "test_data['q2_processed'] = test_data['question2'].apply(lambda x: re.sub(r\"\\'s\", \" \",str(x)))\n",
    "test_data['q2_processed'] = test_data['question2'].apply(lambda x: re.sub(r\"\\'ve\", \" have \",str(x)))\n",
    "test_data['q2_processed'] = test_data['question2'].apply(lambda x: re.sub(r\"can't\", \"cannot \",str(x)))\n",
    "test_data['q2_processed'] = test_data['question2'].apply(lambda x: re.sub(r\"n't\", \" not \",str(x)))\n",
    "test_data['q2_processed'] = test_data['question2'].apply(lambda x: re.sub(r\"i'm\", \"i am \",str(x)))\n",
    "test_data['q2_processed'] = test_data['question2'].apply(lambda x: re.sub(r\"\\'re\", \" are \",str(x)))\n",
    "test_data['q2_processed'] = test_data['question2'].apply(lambda x: re.sub(r\"\\'d\", \" would \",str(x)))\n",
    "test_data['q2_processed'] = test_data['question2'].apply(lambda x: re.sub(r\"\\'ll\", \" will \",str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove all non-alphabet and non-numeric characters\n",
    "train_data['q1_processed'] = train_data['q1_processed'].apply(lambda x: re.sub(r'[^\\w+]',\" \",str(x)))\n",
    "train_data['q2_processed'] = train_data['q2_processed'].apply(lambda x: re.sub(r'[^\\w+]',\" \",str(x)))\n",
    "\n",
    "test_data['q1_processed'] = test_data['q1_processed'].apply(lambda x: re.sub(r'[^\\w+]',\" \",str(x)))\n",
    "test_data['q2_processed'] = test_data['q2_processed'].apply(lambda x: re.sub(r'[^\\w+]',\" \",str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert to lowercase and split into words\n",
    "train_data['q1_processed'] = train_data['q1_processed'].apply(lambda x: str(x).lower().split())\n",
    "train_data['q2_processed'] = train_data['q2_processed'].apply(lambda x: str(x).lower().split())\n",
    "\n",
    "test_data['q1_processed'] = test_data['q1_processed'].apply(lambda x: str(x).lower().split())\n",
    "test_data['q2_processed'] = test_data['q2_processed'].apply(lambda x: str(x).lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "stopwords_en = set(stopwords.words('english'))\n",
    "train_data['q1_processed'] = train_data['q1_processed'].apply(lambda x: [w for w in x if w not in stopwords_en])\n",
    "train_data['q2_processed'] = train_data['q2_processed'].apply(lambda x: [w for w in x if w not in stopwords_en])\n",
    "\n",
    "test_data['q1_processed'] = test_data['q1_processed'].apply(lambda x: [w for w in x if w not in stopwords_en])\n",
    "test_data['q2_processed'] = test_data['q2_processed'].apply(lambda x: [w for w in x if w not in stopwords_en])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of processed q1: 54\n",
      "Max length of processed q2: 103\n",
      "99 percentile length of processed q1: 16\n",
      "99 percentile length of processed q1: 16\n"
     ]
    }
   ],
   "source": [
    "print(\"Max length of processed q1: %d\" % max(train_data['q1_processed'].apply(len)))\n",
    "print(\"Max length of processed q2: %d\" % max(train_data['q2_processed'].apply(len)))\n",
    "print(\"99 percentile length of processed q1: %d\" % np.percentile(train_data['q1_processed'].apply(len),99))\n",
    "print(\"99 percentile length of processed q1: %d\" % np.percentile(train_data['q1_processed'].apply(len),99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Join words to form a single string\n",
    "train_data['q1_processed'] = train_data['q1_processed'].apply(lambda x: \" \".join(x))\n",
    "train_data['q2_processed'] = train_data['q2_processed'].apply(lambda x: \" \".join(x))\n",
    "\n",
    "test_data['q1_processed'] = test_data['q1_processed'].apply(lambda x: \" \".join(x))\n",
    "test_data['q2_processed'] = test_data['q2_processed'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Collate questions in a list\n",
    "q1_list = train_data['q1_processed'].tolist()\n",
    "q2_list = train_data['q2_processed'].tolist()\n",
    "\n",
    "q1_list_test = test_data['q1_processed'].tolist()\n",
    "q2_list_test = test_data['q2_processed'].tolist()\n",
    "\n",
    "all_q_list = q1_list + q2_list + q1_list_test + q2_list_test\n",
    "is_duplicate = train_data['is_duplicate'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of word_index: 121328\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(all_q_list)\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Length of word_index: %d\" % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q1_word_seq = tokenizer.texts_to_sequences(q1_list)\n",
    "q2_word_seq = tokenizer.texts_to_sequences(q2_list)\n",
    "q1_word_seq_padded = pad_sequences(q1_word_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "q2_word_seq_padded = pad_sequences(q2_word_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "y_train = np.array(is_duplicate, dtype=int)\n",
    "\n",
    "q1_word_seq_test = tokenizer.texts_to_sequences(q1_list_test)\n",
    "q2_word_seq_test = tokenizer.texts_to_sequences(q2_list_test)\n",
    "q1_word_seq_padded_test = pad_sequences(q1_word_seq_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "q2_word_seq_padded_test = pad_sequences(q2_word_seq_test, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings: 2196016\n"
     ]
    }
   ],
   "source": [
    "# Obtain GloVe embeddings file from \"http://nlp.stanford.edu/data/glove.840B.300d.zip\"\n",
    "# Create a dictionary with word:vector as key:value pair\n",
    "embed_dict = {}\n",
    "with open(\"glove.840B.300d.txt\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        temp_list = line.split(' ')\n",
    "        word = temp_list[0]\n",
    "        embed_vec = np.asarray(temp_list[1:], dtype='float32')\n",
    "        embed_dict[word] = embed_vec\n",
    "\n",
    "print('Word embeddings: %d' % len(embed_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(121329, 300)\n",
      "Null word embeddings: 33946\n"
     ]
    }
   ],
   "source": [
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embed_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in embed_dict:\n",
    "        embed_matrix[i] = embed_dict[word]\n",
    "\n",
    "print(embed_matrix.shape)\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embed_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump([q1_list,q2_list,q1_list_test,q2_list_test], open(\"q_list.p\", \"wb\"))\n",
    "pickle.dump(embed_dict, open(\"embed_dict.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Lambda, Reshape, Flatten, TimeDistributed, BatchNormalization\n",
    "from keras.layers import concatenate, GlobalAveragePooling2D, Conv2D, MaxPooling2D, dot\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import backend as K\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sq_diff(pair_of_tensors):\n",
    "    x, y = pair_of_tensors\n",
    "    return K.square(x - y)\n",
    "\n",
    "def abs_diff(pair_of_tensors):\n",
    "    x, y = pair_of_tensors\n",
    "    return K.abs(x - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_9 (InputLayer)             (None, 25)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_10 (InputLayer)            (None, 25)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)          (None, 25, 300)       36398700    input_9[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)         (None, 25, 300)       36398700    input_10[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lambda_17 (Lambda)               (None, 25, 300)       0           embedding_9[0][0]                \n",
      "                                                                   embedding_10[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "lambda_18 (Lambda)               (None, 25, 300)       0           embedding_9[0][0]                \n",
      "                                                                   embedding_10[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "reshape_9 (Reshape)              (None, 25, 300, 1)    0           lambda_17[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "reshape_10 (Reshape)             (None, 25, 300, 1)    0           lambda_18[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)               (None, 21, 296, 64)   1664        reshape_9[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)               (None, 21, 296, 64)   1664        reshape_10[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling2D)  (None, 10, 148, 64)   0           conv2d_25[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling2D)  (None, 10, 148, 64)   0           conv2d_28[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)               (None, 7, 145, 64)    65600       max_pooling2d_17[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)               (None, 7, 145, 64)    65600       max_pooling2d_19[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling2D)  (None, 3, 72, 64)     0           conv2d_26[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling2D)  (None, 3, 72, 64)     0           conv2d_29[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)               (None, 1, 70, 64)     36928       max_pooling2d_18[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)               (None, 1, 70, 64)     36928       max_pooling2d_20[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_9 (TimeDistribu (None, 25, 300)       90300       embedding_9[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_10 (TimeDistrib (None, 25, 300)       90300       embedding_10[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_9 (Glob (None, 64)            0           conv2d_27[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_10 (Glo (None, 64)            0           conv2d_30[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "lambda_19 (Lambda)               (None, 300)           0           time_distributed_9[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "lambda_20 (Lambda)               (None, 300)           0           time_distributed_10[0][0]        \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)      (None, 728)           0           global_average_pooling2d_9[0][0] \n",
      "                                                                   global_average_pooling2d_10[0][0]\n",
      "                                                                   lambda_19[0][0]                  \n",
      "                                                                   lambda_20[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_26 (Dense)                 (None, 256)           186624      concatenate_4[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)             (None, 256)           0           dense_26[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNor (None, 256)           1024        dropout_13[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_27 (Dense)                 (None, 256)           65792       batch_normalization_13[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)             (None, 256)           0           dense_27[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNor (None, 256)           1024        dropout_14[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_28 (Dense)                 (None, 256)           65792       batch_normalization_14[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)             (None, 256)           0           dense_28[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNor (None, 256)           1024        dropout_15[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_29 (Dense)                 (None, 256)           65792       batch_normalization_15[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)             (None, 256)           0           dense_29[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNor (None, 256)           1024        dropout_16[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_30 (Dense)                 (None, 1)             257         batch_normalization_16[0][0]     \n",
      "====================================================================================================\n",
      "Total params: 73,574,737\n",
      "Trainable params: 775,289\n",
      "Non-trainable params: 72,799,448\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "question1 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "question2 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "\n",
    "q1 = Embedding(nb_words + 1, \n",
    "                 EMBEDDING_DIM, \n",
    "                 weights=[embed_matrix], \n",
    "                 input_length=MAX_SEQUENCE_LENGTH, \n",
    "                 trainable=False)(question1)\n",
    "\n",
    "\n",
    "\n",
    "q2 = Embedding(nb_words + 1, \n",
    "                 EMBEDDING_DIM, \n",
    "                 weights=[embed_matrix], \n",
    "                 input_length=MAX_SEQUENCE_LENGTH, \n",
    "                 trainable=False)(question2)\n",
    "\n",
    "square_diff = Lambda(sq_diff)([q1, q2])\n",
    "square_diff = Reshape((MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, 1))(square_diff)\n",
    "\n",
    "square_diff = Conv2D(filters=64, kernel_size=5, strides=1, activation='relu')(square_diff)\n",
    "square_diff = MaxPooling2D(pool_size=2)(square_diff)\n",
    "square_diff = Conv2D(filters=64, kernel_size=4, strides=1, activation='relu')(square_diff)\n",
    "square_diff = MaxPooling2D(pool_size=2)(square_diff)\n",
    "square_diff = Conv2D(filters=64, kernel_size=3, strides=1, activation='relu')(square_diff)\n",
    "square_diff = GlobalAveragePooling2D()(square_diff)\n",
    "\n",
    "absolute_diff = Lambda(abs_diff)([q1, q2])\n",
    "absolute_diff = Reshape((MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, 1))(absolute_diff)\n",
    "\n",
    "absolute_diff = Conv2D(filters=64, kernel_size=5, strides=1, activation='relu')(absolute_diff)\n",
    "absolute_diff = MaxPooling2D(pool_size=2)(absolute_diff)\n",
    "absolute_diff = Conv2D(filters=64, kernel_size=4, strides=1, activation='relu')(absolute_diff)\n",
    "absolute_diff = MaxPooling2D(pool_size=2)(absolute_diff)\n",
    "absolute_diff = Conv2D(filters=64, kernel_size=3, strides=1, activation='relu')(absolute_diff)\n",
    "absolute_diff = GlobalAveragePooling2D()(absolute_diff)\n",
    "\n",
    "q1 = TimeDistributed(Dense(EMBEDDING_DIM, activation='relu'))(q1)\n",
    "q2 = TimeDistributed(Dense(EMBEDDING_DIM, activation='relu'))(q2)\n",
    "q1_max = Lambda(lambda x: K.max(x, axis=1), output_shape=(EMBEDDING_DIM, ))(q1)\n",
    "q2_max = Lambda(lambda x: K.max(x, axis=1), output_shape=(EMBEDDING_DIM, ))(q2)\n",
    "\n",
    "ensemble = concatenate([square_diff,absolute_diff,q1_max,q2_max])\n",
    "\n",
    "ensemble = Dense(256, activation='relu')(ensemble)\n",
    "ensemble = Dropout(0.1)(ensemble)\n",
    "ensemble = BatchNormalization()(ensemble)\n",
    "ensemble = Dense(256, activation='relu')(ensemble)\n",
    "ensemble = Dropout(0.1)(ensemble)\n",
    "ensemble = BatchNormalization()(ensemble)\n",
    "ensemble = Dense(256, activation='relu')(ensemble)\n",
    "ensemble = Dropout(0.1)(ensemble)\n",
    "ensemble = BatchNormalization()(ensemble)\n",
    "ensemble = Dense(256, activation='relu')(ensemble)\n",
    "ensemble = Dropout(0.1)(ensemble)\n",
    "ensemble = BatchNormalization()(ensemble)\n",
    "\n",
    "is_duplicate = Dense(1, activation='sigmoid')(ensemble)\n",
    "\n",
    "model = Model(inputs=[question1,question2], outputs=is_duplicate)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 323432 samples, validate on 80858 samples\n",
      "Epoch 1/10\n",
      "Epoch 00000: val_loss improved from inf to 0.48194, saving model to quora_q_pairs.cnn_model_ensemble.best.hdf5\n",
      "249s - loss: 0.5152 - acc: 0.7388 - val_loss: 0.4819 - val_acc: 0.7541\n",
      "Epoch 2/10\n",
      "Epoch 00001: val_loss improved from 0.48194 to 0.45037, saving model to quora_q_pairs.cnn_model_ensemble.best.hdf5\n",
      "249s - loss: 0.4589 - acc: 0.7753 - val_loss: 0.4504 - val_acc: 0.7775\n",
      "Epoch 3/10\n",
      "Epoch 00002: val_loss improved from 0.45037 to 0.43529, saving model to quora_q_pairs.cnn_model_ensemble.best.hdf5\n",
      "253s - loss: 0.4322 - acc: 0.7917 - val_loss: 0.4353 - val_acc: 0.7891\n",
      "Epoch 4/10\n",
      "Epoch 00003: val_loss improved from 0.43529 to 0.42435, saving model to quora_q_pairs.cnn_model_ensemble.best.hdf5\n",
      "254s - loss: 0.4101 - acc: 0.8045 - val_loss: 0.4244 - val_acc: 0.7936\n",
      "Epoch 5/10\n",
      "Epoch 00004: val_loss did not improve\n",
      "252s - loss: 0.3919 - acc: 0.8157 - val_loss: 0.4342 - val_acc: 0.7915\n",
      "Epoch 6/10\n",
      "Epoch 00005: val_loss improved from 0.42435 to 0.42106, saving model to quora_q_pairs.cnn_model_ensemble.best.hdf5\n",
      "255s - loss: 0.3752 - acc: 0.8247 - val_loss: 0.4211 - val_acc: 0.8016\n",
      "Epoch 7/10\n",
      "Epoch 00006: val_loss improved from 0.42106 to 0.41146, saving model to quora_q_pairs.cnn_model_ensemble.best.hdf5\n",
      "254s - loss: 0.3585 - acc: 0.8344 - val_loss: 0.4115 - val_acc: 0.8066\n",
      "Epoch 8/10\n",
      "Epoch 00007: val_loss did not improve\n",
      "252s - loss: 0.3445 - acc: 0.8420 - val_loss: 0.4339 - val_acc: 0.7996\n",
      "Epoch 9/10\n",
      "Epoch 00008: val_loss did not improve\n",
      "252s - loss: 0.3323 - acc: 0.8490 - val_loss: 0.4148 - val_acc: 0.8078\n",
      "Epoch 10/10\n",
      "Epoch 00009: val_loss did not improve\n",
      "252s - loss: 0.3191 - acc: 0.8552 - val_loss: 0.4564 - val_acc: 0.8082\n"
     ]
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='quora_q_pairs.cnn_model_ensemble.best.hdf5',verbose=1, save_best_only=True)\n",
    "\n",
    "results = model.fit([q1_word_seq_padded,q2_word_seq_padded], y_train, batch_size=128, epochs=10,\n",
    "          validation_split=0.2, callbacks=[checkpointer],\n",
    "          verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add cosine similarity feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_7 (InputLayer)             (None, 25)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_8 (InputLayer)             (None, 25)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)          (None, 25, 300)       36398700    input_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)          (None, 25, 300)       36398700    input_8[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)               (None, 25, 300)       0           embedding_7[0][0]                \n",
      "                                                                   embedding_8[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)               (None, 25, 300)       0           embedding_7[0][0]                \n",
      "                                                                   embedding_8[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)              (None, 25, 300, 1)    0           lambda_13[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "reshape_8 (Reshape)              (None, 25, 300, 1)    0           lambda_14[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)               (None, 21, 296, 64)   1664        reshape_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)               (None, 21, 296, 64)   1664        reshape_8[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling2D)  (None, 10, 148, 64)   0           conv2d_19[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling2D)  (None, 10, 148, 64)   0           conv2d_22[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)               (None, 7, 145, 64)    65600       max_pooling2d_13[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)               (None, 7, 145, 64)    65600       max_pooling2d_15[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling2D)  (None, 3, 72, 64)     0           conv2d_20[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling2D)  (None, 3, 72, 64)     0           conv2d_23[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistribu (None, 25, 300)       90300       embedding_7[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "time_distributed_8 (TimeDistribu (None, 25, 300)       90300       embedding_8[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)               (None, 1, 70, 64)     36928       max_pooling2d_14[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)               (None, 1, 70, 64)     36928       max_pooling2d_16[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)               (None, 300)           0           time_distributed_7[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)               (None, 300)           0           time_distributed_8[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_7 (Glob (None, 64)            0           conv2d_21[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_8 (Glob (None, 64)            0           conv2d_24[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dot_2 (Dot)                      (None, 1)             0           lambda_15[0][0]                  \n",
      "                                                                   lambda_16[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)      (None, 729)           0           global_average_pooling2d_7[0][0] \n",
      "                                                                   global_average_pooling2d_8[0][0] \n",
      "                                                                   lambda_15[0][0]                  \n",
      "                                                                   lambda_16[0][0]                  \n",
      "                                                                   dot_2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "dense_19 (Dense)                 (None, 256)           186880      concatenate_3[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)              (None, 256)           0           dense_19[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNorm (None, 256)           1024        dropout_9[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_20 (Dense)                 (None, 256)           65792       batch_normalization_9[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)             (None, 256)           0           dense_20[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNor (None, 256)           1024        dropout_10[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_21 (Dense)                 (None, 256)           65792       batch_normalization_10[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)             (None, 256)           0           dense_21[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNor (None, 256)           1024        dropout_11[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_22 (Dense)                 (None, 256)           65792       batch_normalization_11[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)             (None, 256)           0           dense_22[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNor (None, 256)           1024        dropout_12[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_23 (Dense)                 (None, 1)             257         batch_normalization_12[0][0]     \n",
      "====================================================================================================\n",
      "Total params: 73,574,993\n",
      "Trainable params: 775,545\n",
      "Non-trainable params: 72,799,448\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "question1 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "question2 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "\n",
    "q1 = Embedding(nb_words + 1, \n",
    "                 EMBEDDING_DIM, \n",
    "                 weights=[embed_matrix], \n",
    "                 input_length=MAX_SEQUENCE_LENGTH, \n",
    "                 trainable=False)(question1)\n",
    "\n",
    "\n",
    "\n",
    "q2 = Embedding(nb_words + 1, \n",
    "                 EMBEDDING_DIM, \n",
    "                 weights=[embed_matrix], \n",
    "                 input_length=MAX_SEQUENCE_LENGTH, \n",
    "                 trainable=False)(question2)\n",
    "\n",
    "square_diff = Lambda(sq_diff)([q1, q2])\n",
    "square_diff = Reshape((MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, 1))(square_diff)\n",
    "\n",
    "square_diff = Conv2D(filters=64, kernel_size=5, strides=1, activation='relu')(square_diff)\n",
    "square_diff = MaxPooling2D(pool_size=2)(square_diff)\n",
    "square_diff = Conv2D(filters=64, kernel_size=4, strides=1, activation='relu')(square_diff)\n",
    "square_diff = MaxPooling2D(pool_size=2)(square_diff)\n",
    "square_diff = Conv2D(filters=64, kernel_size=3, strides=1, activation='relu')(square_diff)\n",
    "square_diff = GlobalAveragePooling2D()(square_diff)\n",
    "\n",
    "absolute_diff = Lambda(abs_diff)([q1, q2])\n",
    "absolute_diff = Reshape((MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, 1))(absolute_diff)\n",
    "\n",
    "absolute_diff = Conv2D(filters=64, kernel_size=5, strides=1, activation='relu')(absolute_diff)\n",
    "absolute_diff = MaxPooling2D(pool_size=2)(absolute_diff)\n",
    "absolute_diff = Conv2D(filters=64, kernel_size=4, strides=1, activation='relu')(absolute_diff)\n",
    "absolute_diff = MaxPooling2D(pool_size=2)(absolute_diff)\n",
    "absolute_diff = Conv2D(filters=64, kernel_size=3, strides=1, activation='relu')(absolute_diff)\n",
    "absolute_diff = GlobalAveragePooling2D()(absolute_diff)\n",
    "\n",
    "q1 = TimeDistributed(Dense(EMBEDDING_DIM, activation='relu'))(q1)\n",
    "q2 = TimeDistributed(Dense(EMBEDDING_DIM, activation='relu'))(q2)\n",
    "q1_max = Lambda(lambda x: K.max(x, axis=1), output_shape=(EMBEDDING_DIM, ))(q1)\n",
    "q2_max = Lambda(lambda x: K.max(x, axis=1), output_shape=(EMBEDDING_DIM, ))(q2)\n",
    "\n",
    "q_dot = dot([q1_max,q2_max], [1,1], normalize=True)\n",
    "\n",
    "ensemble = concatenate([square_diff,absolute_diff,q1_max,q2_max,q_dot])\n",
    "\n",
    "ensemble = Dense(256, activation='relu')(ensemble)\n",
    "ensemble = Dropout(0.1)(ensemble)\n",
    "ensemble = BatchNormalization()(ensemble)\n",
    "ensemble = Dense(256, activation='relu')(ensemble)\n",
    "ensemble = Dropout(0.1)(ensemble)\n",
    "ensemble = BatchNormalization()(ensemble)\n",
    "ensemble = Dense(256, activation='relu')(ensemble)\n",
    "ensemble = Dropout(0.1)(ensemble)\n",
    "ensemble = BatchNormalization()(ensemble)\n",
    "ensemble = Dense(256, activation='relu')(ensemble)\n",
    "ensemble = Dropout(0.1)(ensemble)\n",
    "ensemble = BatchNormalization()(ensemble)\n",
    "\n",
    "is_duplicate = Dense(1, activation='sigmoid')(ensemble)\n",
    "\n",
    "model = Model(inputs=[question1,question2], outputs=is_duplicate)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 323432 samples, validate on 80858 samples\n",
      "Epoch 1/10\n",
      "Epoch 00000: val_loss improved from inf to 0.46553, saving model to quora_q_pairs.cnn_model_ensembleII.best.hdf5\n",
      "252s - loss: 0.5133 - acc: 0.7403 - val_loss: 0.4655 - val_acc: 0.7713\n",
      "Epoch 2/10\n",
      "Epoch 00001: val_loss improved from 0.46553 to 0.45784, saving model to quora_q_pairs.cnn_model_ensembleII.best.hdf5\n",
      "253s - loss: 0.4556 - acc: 0.7766 - val_loss: 0.4578 - val_acc: 0.7786\n",
      "Epoch 3/10\n",
      "Epoch 00002: val_loss improved from 0.45784 to 0.43322, saving model to quora_q_pairs.cnn_model_ensembleII.best.hdf5\n",
      "255s - loss: 0.4270 - acc: 0.7935 - val_loss: 0.4332 - val_acc: 0.7878\n",
      "Epoch 4/10\n",
      "Epoch 00003: val_loss did not improve\n",
      "254s - loss: 0.4061 - acc: 0.8072 - val_loss: 0.4366 - val_acc: 0.7820\n",
      "Epoch 5/10\n",
      "Epoch 00004: val_loss improved from 0.43322 to 0.41360, saving model to quora_q_pairs.cnn_model_ensembleII.best.hdf5\n",
      "256s - loss: 0.3857 - acc: 0.8185 - val_loss: 0.4136 - val_acc: 0.8025\n",
      "Epoch 6/10\n",
      "Epoch 00005: val_loss improved from 0.41360 to 0.41210, saving model to quora_q_pairs.cnn_model_ensembleII.best.hdf5\n",
      "256s - loss: 0.3679 - acc: 0.8281 - val_loss: 0.4121 - val_acc: 0.8031\n",
      "Epoch 7/10\n",
      "Epoch 00006: val_loss did not improve\n",
      "254s - loss: 0.3525 - acc: 0.8378 - val_loss: 0.4391 - val_acc: 0.7843\n",
      "Epoch 8/10\n",
      "Epoch 00007: val_loss did not improve\n",
      "255s - loss: 0.3382 - acc: 0.8451 - val_loss: 0.4201 - val_acc: 0.8062\n",
      "Epoch 9/10\n",
      "Epoch 00008: val_loss did not improve\n",
      "253s - loss: 0.3240 - acc: 0.8526 - val_loss: 0.4126 - val_acc: 0.8096\n",
      "Epoch 10/10\n",
      "Epoch 00009: val_loss did not improve\n",
      "253s - loss: 0.3122 - acc: 0.8586 - val_loss: 0.4200 - val_acc: 0.8054\n"
     ]
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='quora_q_pairs.cnn_model_ensembleII.best.hdf5',verbose=1, save_best_only=True)\n",
    "\n",
    "results = model.fit([q1_word_seq_padded,q2_word_seq_padded], y_train, batch_size=128, epochs=10,\n",
    "          validation_split=0.2, callbacks=[checkpointer],\n",
    "          verbose=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Probabilities of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('quora_q_pairs.cnn_model_ensemble.best.hdf5')\n",
    "p = model.predict([q1_word_seq_padded_test,q2_word_seq_padded_test], batch_size=128, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.079556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.240708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.577627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.026925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.001609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_id  is_duplicate\n",
       "0        0      0.079556\n",
       "1        1      0.240708\n",
       "2        2      0.577627\n",
       "3        3      0.026925\n",
       "4        4      0.001609"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pd.DataFrame(p)\n",
    "output.columns = ['is_duplicate']\n",
    "output['test_id'] = output.index\n",
    "output = output[['test_id','is_duplicate']]\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv(\"prediction.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
